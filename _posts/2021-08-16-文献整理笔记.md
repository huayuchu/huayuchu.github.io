---
layout: post
comments: false
title: "Related work of event-triggered control with reinforcement learning"
date: 2021-08-16 16:40:00
---

## 文献整理 | rlEventTrig

> 以下内容来自google scholar。

---

1

Xiangnan, 2014 (IJCNN)

```tex
@inproceedings{zhong2014event,
  title={Event-triggered reinforcement learning approach for unknown nonlinear continuous-time system},
  author={Zhong, Xiangnan and Ni, Zhen and He, Haibo and Xu, Xin and Zhao, Dongbin},
  booktitle={2014 International Joint Conference on Neural Networks (IJCNN)},
  pages={3677--3684},
  year={2014},
  organization={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/6889787

This paper provides an adaptive event-triggered method using adaptive dynamic programming (ADP) for the nonlinear continuous-time system. Comparing to the traditional method with fixed sampling period, the event-triggered method samples the state only when an event is triggered and therefore the computational cost is reduced. We demonstrate the theoretical analysis on the stability of the event-triggered method, and integrate it with the ADP approach. The system dynamics are assumed unknown. The corresponding ADP algorithm is given and the neural network techniques are applied to implement this method. The simulation results verify the theoretical analysis and justify the efficiency of the proposed event-triggered technique using the ADP approach.

本文为非线性连续时间系统提供了一种使用自适应动态规划（ADP）的自适应事件触发方法。 与固定采样周期的传统方法相比，事件触发方法仅在事件被触发时才对状态进行采样，从而降低了计算成本。 我们展示了事件触发方法稳定性的理论分析，并将其与 ADP 方法相结合。 假设系统动力学未知。 给出了相应的ADP算法，并应用神经网络技术来实现该方法。 仿真结果验证了理论分析并证明了使用 ADP 方法所提出的事件触发技术的效率。

- I.

    Introduction

- II.

    Problem Statement

- III.

    Stability Analysis of the Event-Triggered Method

- IV.

    Event-Triggered Controller Design by Neural Network Techniques

- V.

    Simulation Results

- VI.

    Conclusion

---

2

Dominik, 2018 (CDC)

```tex
@inproceedings{baumann2018deep,
  title={Deep reinforcement learning for event-triggered control},
  author={Baumann, Dominik and Zhu, Jia-Jie and Martius, Georg and Trimpe, Sebastian},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={943--950},
  year={2018},
  organization={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/8619335

Event-triggered control (ETC) methods can achieve high-performance control with a significantly lower number of samples compared to usual, time-triggered methods. These frameworks are often based on a mathematical model of the system and specific designs of controller and event trigger. In this paper, we show how deep reinforcement learning (DRL) algorithms can be leveraged to simultaneously learn control and communication behavior from scratch, and present a DRL approach that is particularly suitable for ETC. To our knowledge, this is the first work to apply DRL to ETC. We validate the approach on multiple control tasks and compare it to model-based event-triggering frameworks. In particular, we demonstrate that it can, other than many model-based ETC designs, be straightforwardly applied to nonlinear systems.

与通常的时间触发方法相比，事件触发控制 (ETC) 方法可以实现高性能控制，并且样本数量明显减少。 这些框架通常基于系统的数学模型以及控制器和事件触发器的特定设计。 在本文中，我们展示了如何利用深度强化学习 (DRL) 算法从头开始同时学习控制和通信行为，并提出一种特别适合 ETC 的 DRL 方法。 据我们所知，这是将 DRL 应用于 ETC 的第一项工作。 我们在多个控制任务上验证该方法，并将其与基于模型的事件触发框架进行比较。 特别是，我们证明了它可以直接应用于非线性系统，而不是许多基于模型的 ETC 设计。

- I.

    Introduction

- II.

    Background

- III.

    Approach

- IV.

    Validation

- V.

    Discussion

---

3

Vignesh, 2017 (TC)

```tex
@article{narayanan2017event,
  title={Event-triggered distributed control of nonlinear interconnected systems using online reinforcement learning with exploration},
  author={Narayanan, Vignesh and Jagannathan, Sarangapani},
  journal={IEEE transactions on cybernetics},
  volume={48},
  number={9},
  pages={2510--2519},
  year={2017},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/8027115

In this paper, a distributed control scheme for an interconnected system composed of uncertain input affine nonlinear subsystems with event triggered state feedback is presented by using a novel hybrid learning scheme-based approximate dynamic programming with online exploration. First, an approximate solution to the Hamilton-Jacobi-Bellman equation is generated with event sampled neural network (NN) approximation and subsequently, a near optimal control policy for each subsystem is derived. Artificial NNs are utilized as function approximators to develop a suite of identifiers and learn the dynamics of each subsystem. The NN weight tuning rules for the identifier and event-triggering condition are derived using Lyapunov stability theory. Taking into account, the effects of NN approximation of system dynamics and boot-strapping, a novel NN weight update is presented to approximate the optimal value function. Finally, a novel strategy to incorporate exploration in online control framework, using identifiers, is introduced to reduce the overall cost at the expense of additional computations during the initial online learning phase. System states and the NN weight estimation errors are regulated and local uniformly ultimately bounded results are achieved. The analytical results are substantiated using simulation studies.

在本文中，通过使用一种新颖的基于混合学习方案的近似动态规划和在线探索，提出了一种由具有事件触发状态反馈的不确定输入仿射非线性子系统组成的互连系统的分布式控制方案。首先，使用事件采样神经网络 (NN) 近似生成 Hamilton-Jacobi-Bellman 方程的近似解，然后推导出每个子系统的近乎最优的控制策略。人工神经网络被用作函数逼近器来开发一套标识符并学习每个子系统的动态。使用李雅普诺夫稳定性理论推导出标识符和事件触发条件的 NN 权重调整规则。考虑到系统动力学和自举的 NN 逼近的影响，提出了一种新颖的 NN 权重更新来逼近最优值函数。最后，引入了一种使用标识符将探索纳入在线控制框架的新策略，以在初始在线学习阶段以额外计算为代价来降低总体成本。系统状态和 NN 权重估计误差得到调节，并获得局部一致的最终有界结果。使用模拟研究证实了分析结果。

- I.

    Introduction

- II.

    Background and Problem Statement

- III.

    Event Driven Adaptive Dynamic Programming

- IV.

    Learning With Exploration for Online Control

- V.

    Stability Analysis

- VI.

    Simulation Results

- VII.

    Conclusions

---

4

Xinxin, 2019 (TC)

```tex
@article{guo2019event,
  title={Event-triggered reinforcement learning-based adaptive tracking control for completely unknown continuous-time nonlinear systems},
  author={Guo, Xinxin and Yan, Weisheng and Cui, Rongxin},
  journal={IEEE transactions on cybernetics},
  volume={50},
  number={7},
  pages={3231--3242},
  year={2019},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/8677275

In this paper, event-triggered reinforcement learning-based adaptive tracking control is developed for the continuous-time nonlinear system with unknown dynamics and external disturbances. The critic and action neural networks are designed to approximate an unknown long-term performance index and controller, respectively. The dead-zone event-triggered condition is developed to reduce communication and computational costs. Rigorous theoretical analysis is provided to show that the closed-loop system can be stabilized. The weight errors and the filtered tracking error are all uniformly ultimately bounded. Finally, to demonstrate the developed controller, the simulation results are provided using an autonomous underwater vehicle model.

在本文中，针对具有未知动力学和外部扰动的连续时间非线性系统，开发了基于事件触发的强化学习的自适应跟踪控制。 评论家和动作神经网络旨在分别逼近未知的长期性能指标和控制器。 开发死区事件触发条件是为了减少通信和计算成本。 提供了严格的理论分析来表明闭环系统可以稳定。 权重误差和过滤后的跟踪误差都是一致最终有界的。 最后，为了演示开发的控制器，使用自主水下航行器模型提供了仿真结果。

---

5

Ashkan, 2020 (AE)

```tex
@article{hosseinloo2020data,
  title={Data-driven control of micro-climate in buildings: An event-triggered reinforcement learning approach},
  author={Hosseinloo, Ashkan Haji and Ryzhov, Alexander and Bischi, Aldo and Ouerdane, Henni and Turitsyn, Konstantin and Dahleh, Munther A},
  journal={Applied Energy},
  volume={277},
  pages={115451},
  year={2020},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0306261920309636

Smart buildings have great potential for shaping an energy-efficient, sustainable, and more economic future for our planet as buildings account for approximately 40% of the global energy consumption. Future of the smart buildings lies in using sensory data for adaptive decision making and control that is currently gloomed by the key challenge of learning a good control policy in a short period of time in an online and continuing fashion. To tackle this challenge, an event-triggered – as opposed to classic time-triggered – paradigm, is proposed in which learning and control decisions are made when events occur and enough information is collected. Events are characterized by certain design conditions and they occur when the conditions are met, for instance, when a certain state threshold is reached. By systematically adjusting the time of learning and control decisions, the proposed framework can potentially reduce the variance in learning, and consequently, improve the control process. We formulate the micro-climate control problem based on semi-Markov decision processes that allow for variable-time state transitions and decision making. Using extended policy gradient theorems and temporal difference methods in a reinforcement learning set-up, we propose two learning algorithms for event-triggered control of micro-climate in buildings. We show the efficacy of our proposed approach via designing a smart learning thermostat that simultaneously optimizes energy consumption and occupants’ comfort in a test building.

智能建筑在为我们的星球塑造一个节能、可持续和更经济的未来方面具有巨大的潜力，因为建筑约占全球能源消耗的 40%。智能建筑的未来在于使用传感数据进行自适应决策和控制，目前，以在线和持续方式在短时间内学习良好的控制策略的关键挑战使这一点黯然失色。为了应对这一挑战，提出了一种事件触发——而不是经典的时间触发——范式，其中在事件发生并收集到足够的信息时做出学习和控制决策。事件以特定的设计条件为特征，并且在满足条件时发生，例如，当达到某个状态阈值时。通过系统地调整学习和控制决策的时间，所提出的框架可以潜在地减少学习的方差，从而改善控制过程。我们基于允许可变时间状态转换和决策制定的半马尔可夫决策过程来制定微气候控制问题。在强化学习设置中使用扩展的策略梯度定理和时间差异方法，我们提出了两种用于事件触发控制建筑物微气候的学习算法。我们通过设计一个智能学习恒温器来展示我们提出的方法的有效性，该恒温器可以同时优化测试建筑中的能源消耗和居住者的舒适度。

1. Introduction
2. Related work and contributions
3. Problem statement
4. SMDP framework
5. Reinforcement learning algorithm and implementation
6. Simulations and results
7. Conclusion
8. Declaration of Competing Interest
9. Acknowledgements
10. References

---

6

Xiong, 2017 (TSMC:S)

```tex
@article{yang2017event,
  title={Event-triggered optimal neuro-controller design with reinforcement learning for unknown nonlinear systems},
  author={Yang, Xiong and He, Haibo and Liu, Derong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={49},
  number={9},
  pages={1866--1878},
  year={2017},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/8183439

This paper develops an optimal control scheme for continuous-time unknown nonlinear systems using the event-triggering mechanism. Different from designing controllers using the time-triggering mechanism, the event-triggered controller is updated only when the system state deviates more than a certain threshold from a prescribed value. To obtain the event-triggered optimal controller, we develop an identifier-critic architecture under the framework of reinforcement learning. The identifier network, composed of a feedforward neural network (FNN), aims to derive the knowledge of unknown system dynamics, and the critic network, constituted of an FNN, intends to derive the event-triggered optimal controller. The identifier network is tuned via the combination of a standard back-propagation algorithm and an e-modification method, and the critic network is updated using a modification of the gradient descent method. By introducing an additional stability term to update the critic network, the initial admissible control is no longer required. Meanwhile, by using historical and instantaneous state data together, the persistence of excitation condition is relaxed. A stability analysis of the closed-loop system is provided based on the Lyapunov method. The effectiveness of the proposed designs is illustrated through simulations of a nonlinear example and a single link robot arm system.

本文使用事件触发机制为连续时间未知非线性系统开发了一种最优控制方案。与使用时间触发机制设计控制器不同，事件触发控制器仅在系统状态偏离规定值超过某个阈值时才更新。为了获得事件触发的最优控制器，我们在强化学习的框架下开发了一个标识符批评架构。由前馈神经网络 (FNN) 组成的标识符网络旨在推导出未知系统动力学的知识，而由 FNN 构成的评论网络旨在推导出事件触发的最优控制器。标识符网络通过标准反向传播算法和 e 修改方法的组合进行调整，而评论网络则使用梯度下降方法的修改进行更新。通过引入额外的稳定性项来更新评论家网络，不再需要初始的可接受控制。同时，通过结合使用历史和瞬时状态数据，放松了激励条件的持续性。基于李雅普诺夫方法提供了闭环系统的稳定性分析。通过对非线性示例和单连杆机械臂系统的模拟，说明了所提出设计的有效性。

- I.

    Introduction

- II.

    Problem Statements and Preliminaries

- III.

    Event-Triggered Optimal Control Scheme

- IV.

    Identifier-Critic Architecture to Solve the Event-Triggered HJB Equation

- V.

    Stability Analysis

- VI.

    Simulation Results

- VII.

    Conclusion

---

7

Shan, 2020 (NN)

```tex
@article{xue2020integral,
  title={Integral reinforcement learning based event-triggered control with input saturation},
  author={Xue, Shan and Luo, Biao and Liu, Derong},
  journal={Neural Networks},
  volume={131},
  pages={144--153},
  year={2020},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0893608020302574

In this paper, a novel integral reinforcement learning (IRL)-based event-triggered adaptive dynamic programming scheme is developed for input-saturated continuous-time nonlinear systems. By using the IRL technique, the learning system does not require the knowledge of the drift dynamics. Then, a single critic neural network is designed to approximate the unknown value function and its learning is not subjected to the requirement of an initial admissible control. In order to reduce computational and communication costs, the event-triggered control law is designed. The triggering threshold is given to guarantee the asymptotic stability of the control system. Two examples are employed in the simulation studies, and the results verify the effectiveness of the developed IRL-based event-triggered control method.

在本文中，为输入饱和的连续时间非线性系统开发了一种新的基于积分强化学习 (IRL) 的事件触发自适应动态规划方案。 通过使用 IRL 技术，学习系统不需要漂移动力学的知识。 然后，设计了一个单一的批评神经网络来逼近未知值函数，并且它的学习不受初始可接受控制的要求。 为了降低计算和通信成本，设计了事件触发控制律。 给出触发阈值以保证控制系统的渐近稳定性。 在仿真研究中采用了两个例子，结果验证了所开发的基于 IRL 的事件触发控制方法的有效性。

1. Introduction
2. Problem description
3. Analysis of event-triggered control system
    - 3.1. Design of event-triggered control system
    - 3.2. Lower bound of the triggering interval
4. The realization of the scheme based on ADP
    - 4.1. NN realization technique
    - 4.2. Stability analysis
5. Simulation studies
    - 5.1. Example I
    - 5.2. Example II
6. Conclusions
7. Declaration of Competing Interest
8. Acknowledgments
9. References

---

8

Xiong, 2019 (TC)

```tex
@article{yang2019decentralized,
  title={Decentralized event-triggered control for a class of nonlinear-interconnected systems using reinforcement learning},
  author={Yang, Xiong and He, Haibo},
  journal={IEEE transactions on cybernetics},
  volume={51},
  number={2},
  pages={635--648},
  year={2019},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/8884147

In this article, we propose a novel decentralized event-triggered control (ETC) scheme for a class of continuous-time nonlinear systems with matched interconnections. The present interconnected systems differ from most of the existing interconnected plants in that their equilibrium points are no longer assumed to be zero. Initially, we establish a theorem to indicate that the decentralized ETC law for the overall system can be represented by an array of optimal ETC laws for nominal subsystems. Then, to obtain these optimal ETC laws, we develop a reinforcement learning (RL)-based method to solve the Hamilton-Jacobi-Bellman equations arising in the discounted-cost optimal ETC problems of the nominal subsystems. Meanwhile, we only use critic networks to implement the RL-based approach and tune the critic network weight vectors by using the gradient descent method and the concurrent learning technique together. With the proposed weight vectors tuning rule, we are able to not only relax the persistence of the excitation condition but also ensure the critic network weight vectors to be uniformly ultimately bounded. Moreover, by utilizing the Lyapunov method, we prove that the obtained decentralized ETC law can force the entire system to be stable in the sense of uniform ultimate boundedness. Finally, we validate the proposed decentralized ETC strategy through simulations of the nonlinear-interconnected systems derived from two inverted pendulums connected via a spring.

在本文中，我们为一类具有匹配互连的连续时间非线性系统提出了一种新颖的分散事件触发控制 (ETC) 方案。目前的互联系统与大多数现有互联工厂的不同之处在于它们的平衡点不再假设为零。最初，我们建立了一个定理来表明整个系统的分散 ETC 定律可以用标称子系统的一组最优 ETC 定律来表示。然后，为了获得这些最优 ETC 定律，我们开发了一种基于强化学习 (RL) 的方法来求解在标称子系统的贴现成本最优 ETC 问题中出现的 Hamilton-Jacobi-Bellman 方程。同时，我们仅使用评论家网络来实现基于 RL 的方法，并结合使用梯度下降法和并发学习技术来调整评论家网络权重向量。通过提出的权重向量调整规则，我们不仅能够放宽激励条件的持续性，还可以确保评论家网络权重向量最终统一有界。此外，利用李雅普诺夫方法，我们证明了所获得的去中心化 ETC 律可以迫使整个系统在统一极限有界的意义上稳定。最后，我们通过模拟从通过弹簧连接的两个倒立摆导出的非线性互连系统来验证所提出的分散 ETC 策略。

- I.

    Introduction

- II.

    Problem Statement and Preliminaries

- III.

    Decentralized ETC Strategy

- IV.

    Stability Analysis

- V.

    Lower Bound of the Minimal Intersample Time

- VI.

    Simulation Study

- VII.

    Conclusion

- Appendix A

    Proof of Theorem 1

- Appendix B

    Proof of Theorem 2

---

9

Xiaobo, 2020 (Neurocomputing)

```tex
@article{lin2020event,
  title={Event-triggered reinforcement learning control for the quadrotor UAV with actuator saturation},
  author={Lin, Xiaobo and Liu, Jian and Yu, Yao and Sun, Changyin},
  journal={Neurocomputing},
  volume={415},
  pages={135--145},
  year={2020},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0925231220311504

This paper proposes an event-triggered reinforcement learning (RL) control strategy to stabilize the quadrotor unmanned aerial vehicle (UAV) with actuator saturation. As the quadrotor UAV equips with a complex dynamic is difficult to be model accurately, a model free reinforcement learning scheme is designed. Due to the practical limitation of actuators, the end of controller is constrained with a bounded function. In order to reduce the calculation consumption for the onboard computer, an event-triggered mechanism is developed, which only update the controller when the triggered condition is satisfied. The proposed controller is implemented with two neural networks which are called critic and actor. Some advanced RL technologies are utilized for speeding up the train process, e.g. off-policy training, experience replay, etc. The stability of closed-loop system is proved by the Lyapunov analysis. The simulation results including a stability task and a tracking task verify the theoretical analysis, in which we find the updating frequency of controller is decreased greatly.

本文提出了一种事件触发的强化学习 (RL) 控制策略，以在执行器饱和的情况下稳定四旋翼无人机 (UAV)。由于四旋翼无人机配备复杂的动力学难以准确建模，设计了一种无模型强化学习方案。由于执行器的实际限制，控制器的末端受到有界函数的约束。为了减少车载计算机的计算消耗，开发了一种事件触发机制，只有在满足触发条件时才更新控制器。所提出的控制器是用两个神经网络实现的，它们被称为评论家和演员。一些先进的 RL 技术被用于加速训练过程，例如off-policy训练，经验回放等。Lyapunov分析证明了闭环系统的稳定性。包括稳定性任务和跟踪任务的仿真结果验证了理论分析，我们发现控制器的更新频率大大降低。

1. Introduction
2. Quadrotor UAV system model and problem description
3. Event-triggered RL controller designed with constrained control inputs
    - 3.1. Event-triggered regulator designed
    - 3.2. Neural-network based controller designed
    - 3.3. Stability analysis of proposed method
    - 3.4. Minimum inter-event period
4. Simulations
    - 4.1. Simulation setup
    - 4.2. Stability task
    - 4.3. Tracking task
5. Conclusion
6. CRediT author statement
7. Declaration of Competing Interest
8. References

---

10

Hanguang, 2020 (Neurocomputing)

```tex
@article{su2020integral,
  title={Integral reinforcement learning-based online adaptive event-triggered control for non-zero-sum games of partially unknown nonlinear systems},
  author={Su, Hanguang and Zhang, Huaguang and Sun, Shaoxin and Cai, Yuliang},
  journal={Neurocomputing},
  volume={377},
  pages={243--255},
  year={2020},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0925231219314109

This paper develops an integral reinforcement learning (IRL)-based adaptive control method for the multi-player non-zero-sum (NZS) games of the nonlinear continuous-time systems with partially unknown dynamics, in the context of event-triggered mechanism. With the principle of IRL method, the requirement for the system drift dynamics is relaxed in the controller design. Moreover, different from the conventional iteration computation methods, the algorithm developed in this work is implemented in an online adaptive fashion, which provides a new way to combine the IRL algorithm and the event-triggered control framework in solving the NZS game issues. In the event-based algorithm, a state-dependent triggering condition is presented, which not only guarantees the closed-loop system stability, but also reduces the computation and communication loads of the controlled plant. By means of Lyapunov theorem, the uniform ultimate boundedness (UUB) properties of the system states and the critic weight estimation errors have been proved. Finally, two numerical examples are utilized to demonstrate the efficacy of the proposed method.

本文在事件触发机制的背景下，开发了一种基于积分强化学习 (IRL) 的自适应控制方法，用于具有部分未知动力学的非线性连续时间系统的多人非零和 (NZS) 博弈。利用IRL方法的原理，在控制器设计中放宽了对系统漂移动力学的要求。此外，与传统的迭代计算方法不同，本文开发的算法以在线自适应方式实现，为结合 IRL 算法和事件触发控制框架解决 NZS 博弈问题提供了一种新方法。在基于事件的算法中，提出了一个状态相关的触发条件，既保证了闭环系统的稳定性，又减少了被控对象的计算和通信负荷。借助李雅普诺夫定理，证明了系统状态的统一极限有界(UUB)性质和临界权重估计误差。最后，利用两个数值例子来证明所提出方法的有效性。

1. Introduction
2. Problem formulation and preliminaries
    - 2.1. Time-triggered NZS games
3. IRL-based adaptive ETC design for the NZS games
    - 3.1. Formulation of IRL algorithm
    - 3.2. Optimal event-triggered controller design
    - 3.3. Event-based adaptive critic design
    - 3.4. Stability proof of the online ETC system
4. Simulation studies
5. Conclusion and future work
6. Declaration of Competing Interest
7. Acknowledgement
8. References

---

11

Dongsheng, 2019 (Neurocomputing)

```tex
@article{yang2019event,
  title={Event-trigger-based robust control for nonlinear constrained-input systems using reinforcement learning method},
  author={Yang, Dongsheng and Li, Ting and Zhang, Huaguang and Xie, Xiangpeng},
  journal={Neurocomputing},
  volume={340},
  pages={158--170},
  year={2019},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0925231219302486

In this paper, an online integral reinforcement learning strategy is proposed to deal with robust constrained control problems using event-triggered mechanism for nonlinear Continuous-Time (C-T) systems with external disturbances. The novel design of constrained control law is addressed together with the adaptive event-triggered condition by guaranteeing the optimal performance and system stability. An adaptive online actor-critic [Neural Network](https://www.sciencedirect.com/topics/computer-science/neural-networks) (NN) reinforcement learning scheme is developed to approximate the optimal solution of the complicated Hamilton–Jacobi–Isaacs equation. Meanwhile, the convergence of NN weight errors and the event-triggered closed-loop system stability are demonstrated to be uniform ultimate bounded by Lyapunov analysis under the proposed triggering condition. Moreover, event-triggered *H*∞ tracking control with input constrains and limited network communication is also presented by establishing an augmented system. Finally, simulation results are provided to show the algorithm validity.

在本文中，提出了一种在线积分强化学习策略来处理具有外部干扰的非线性连续时间（C-T）系统的事件触发机制的鲁棒约束控制问题。通过保证最佳性能和系统稳定性，约束控制律的新颖设计与自适应事件触发条件一起得到解决。开发了一种自适应在线演员-评论家神经网络 (NN) 强化学习方案，以逼近复杂的 Hamilton-Jacobi-Isaacs 方程的最优解。同时，在所提出的触发条件下，通过李雅普诺夫分析证明了神经网络权重误差的收敛性和事件触发的闭环系统稳定性是一致的极限有界。此外，还通过建立增强系统提出了具有输入约束和有限网络通信的事件触发 H∞ 跟踪控制。最后给出了仿真结果来证明算法的有效性。

1. Introduction
2. Problem formulation
3. Event-trigger-based *H*∞ control design for system with input constrains
    - 3.1. Optimal solution and HJI equation for event-trigger-based *H*∞ control
    - 3.2. Triggering condition with disturbances and control constrains
4. Reinforcement learning with event-triggered ADP
    - 4.1. Critic and actor NN approximation
    - 4.2. Weight update of critic and actor NN
        - 4.2.1. Weight dynamic of critic NN
        - 4.2.2. Weight dynamic of actor NN
    - 4.3. Stability analysis of the closed-loop system with event-trigger-based *H*∞ control
        - 4.3.1. Event are not triggered
        - 4.3.2. In the triggering instant
5. Event-triggered *H*∞ tracking control for constrained-input system
6. Simulation
    - 6.1. Example 1: Event-trigger-based *H*∞ control for nonlinear system
    - 6.2. Example 2: Event-triggered *H*∞ tracking control
        - 6.2.1. Case 1: Linear system simulation
        - 6.2.2. Case 2: Nonlinear system simulation
7. Conclusion

---

12

Yuling, 2021 (Information Sciences)

```tex
@article{liang2021event,
  title={Event-triggered reinforcement learning H∞ control design for constrained-input nonlinear systems subject to actuator failures},
  author={Liang, Yuling and Zhang, Huaguang and Duan, Jie and Sun, Shaoxin},
  journal={Information Sciences},
  volume={543},
  pages={273--295},
  year={2021},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/abs/pii/S0020025520307180

In the paper, a novel input-constrained H∞ fault-tolerant control approach is developed by using sliding mode control technology and event-triggered reinforcement learning (RL) algorithm. To reduce or even eliminate the impacts of the time-varying actuator failures, a properly sliding mode control strategy is proposed for the controlled system, while the event-triggered H∞ control scheme is established via RL algorithm for the equivalent sliding mode dynamics. By utilizing a single neural network (NN), the Hamilton–Jacobi–Bellman (HJB) equation can be solved approximately, thereby gaining time-triggered worst-case disturbance law, as well as event-triggered optimal control policy. Besides, it is unnecessary to given a initial stabilizing control input in the learning process of neural networks (NNs) in this paper. Moreover, the Lyapunov stability principle is applied to guarantee that the controlled system is uniformly ultimately bounded (UUB). Finally, to verify the feasibility and efficient performance of the developed approach, three simulations are carried out.

在本文中，利用滑模控制技术和事件触发强化学习（RL）算法开发了一种新的输入约束容错控制方法。为了减少甚至消除时变执行器故障的影响，对受控系统提出了适当的滑模控制策略，同时通过等效滑模动力学的RL算法建立了事件触发控制方案。通过利用单个神经网络 (NN)，可以近似求解 Hamilton-Jacobi-Bellman (HJB) 方程，从而获得时间触发的最坏情况扰动规律以及事件触发的最优控制策略。此外，本文没有必要在神经网络（NN）的学习过程中给出初始稳定控制输入。此外，应用李雅普诺夫稳定性原理来保证受控系统是一致最终有界（UUB）。最后，为了验证所开发方法的可行性和高效性能，进行了三个仿真。

---

13

Saiwei, 2020 (TNSE)

```tex
@article{wang2020model,
  title={Model-Free Event-Triggered Optimal Consensus Control of Multiple Euler-Lagrange Systems via Reinforcement Learning},
  author={Wang, Saiwei and Jin, Xin and Mao, Shuai and Vasilakos, Athanasios V and Tang, Yang},
  journal={IEEE Transactions on Network Science and Engineering},
  volume={8},
  number={1},
  pages={246--258},
  year={2020},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/9252142

This paper develops a model-free approach to solve the event-triggered optimal consensus of multiple Euler-Lagrange systems (MELSs) via reinforcement learning (RL). Firstly, an augmented system is constructed by defining a pre-compensator to circumvent the dependence on system dynamics. Secondly, the Hamilton-Jacobi-Bellman (HJB) equations are applied to the deduction of the model-free event-triggered optimal controller. Thirdly, we present a policy iteration (PI) algorithm derived from RL, which converges to the optimal policy. Then, the value function of each agent is represented through a neural network to realize the PI algorithm. Moreover, the gradient descent method is used to update the neural network only at a series of discrete event-triggered instants. The specific form of the event-triggered condition is then proposed, and it is guaranteed that the closed-loop augmented system under the event-triggered mechanism is uniformly ultimately bounded (UUB). Meanwhile, the Zeno behavior is also eliminated. Finally, the validity of this approach is verified by a simulation example.

本文开发了一种无模型方法，通过强化学习 (RL) 解决多个欧拉-拉格朗日系统 (MELS) 的事件触发最优一致性问题。首先，通过定义预补偿器来构建增强系统，以规避对系统动力学的依赖。其次，将Hamilton-Jacobi-Bellman(HJB)方程应用于无模型事件触发最优控制器的推导。第三，我们提出了一种从 RL 派生的策略迭代 (PI) 算法，该算法收敛到最优策略。然后通过神经网络表示每个agent的价值函数，实现PI算法。此外，梯度下降法仅用于在一系列离散事件触发时刻更新神经网络。然后提出了事件触发条件的具体形式，保证了事件触发机制下的闭环增强系统是一致最终有界（UUB）。同时，芝诺行为也被消除。最后，通过仿真实例验证了该方法的有效性。

- I.

    Introduction

- II.

    Preliminaries

- III.

    Design of Model-Free Event-Triggered Method

- IV.

    Main Results

- V.

    Simulation Results

- VI.

    Conclusion

---

14

Weiwei, 2021 (TNNLS)

```tex
@article{bai2021event,
  title={Event-Triggered Multigradient Recursive Reinforcement Learning Tracking Control for Multiagent Systems},
  author={Bai, Weiwei and Li, Tieshan and Long, Yue and Chen, CL Philip},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}
```

https://ieeexplore.ieee.org/abstract/document/9488578

In this article, the tracking control problem of event-triggered multigradient recursive reinforcement learning is investigated for nonlinear multiagent systems (MASs). Attention is focused on the distributed reinforcement learning approach for MASs. The critic neural network (NN) is applied to estimate the long-term strategic utility function, and the actor NN is designed to approximate the uncertain dynamics in MASs. The multigradient recursive (MGR) strategy is tailored to learn the weight vector in NN, which eliminates the local optimal problem inherent in gradient descent method and decreases the dependence of initial value. Furthermore, reinforcement learning and event-triggered mechanism can improve the energy conservation of MASs by decreasing the amplitude of the controller signal and the controller update frequency, respectively. It is proved that all signals in MASs are semiglobal uniformly ultimately bounded (SGUUB) according to the Lyapunov theory. Simulation results are given to demonstrate the effectiveness of the proposed strategy.

在本文中，针对非线性多智能体系统 (MAS)，研究了事件触发的多梯度递归强化学习的跟踪控制问题。注意力集中在 MAS 的分布式强化学习方法上。批评神经网络 (NN) 用于估计长期战略效用函数，而actor NN 旨在近似 MAS 中的不确定动态。多梯度递归（MGR）策略专门用于学习NN中的权重向量，消除了梯度下降法固有的局部最优问题，降低了对初始值的依赖性。此外，强化学习和事件触发机制可以通过分别降低控制器信号的幅度和控制器更新频率来提高 MAS 的能量守恒。根据李雅普诺夫理论证明了 MAS 中的所有信号都是半全局一致最终有界（SGUUB）。仿真结果证明了所提出策略的有效性。

---

> 以下内容来自TAC | "reinforcement learning" + "event"

无

---

> 以下内容来自Automatica | "reinforcement learning" + "event"

1

Vamvoudakis, 2018 (Automatica)

```tex
@article{vamvoudakis2018model,
  title={Model-free event-triggered control algorithm for continuous-time linear systems with optimal performance},
  author={Vamvoudakis, Kyriakos G and Ferraz, Henrique},
  journal={Automatica},
  volume={87},
  pages={412--420},
  year={2018},
  publisher={Elsevier}
}
```

https://www.sciencedirect.com/science/article/pii/S000510981730136X

This paper proposes a new model-free event-triggered optimal control algorithm for continuous-time linear systems. The problem is formulated as an infinite-horizon optimal adaptive learning problem, and we are able to simultaneously address the issue of designing a control and a triggering mechanism with guaranteed optimal performance by design. In order to provide a model-free solution, we adopt a [Q-learning](https://www.sciencedirect.com/topics/engineering/q-learning) framework with a critic network to approximate the optimal cost and a zero-order hold actor network to approximate the optimal control. Since we have dynamics that evolve in continuous and discrete-time, we write the [closed-loop system](https://www.sciencedirect.com/topics/engineering/feedback-control-systems) as an impulsive model and prove [asymptotic stability](https://www.sciencedirect.com/topics/engineering/asymptotic-stability) of its equilibrium. Numerical simulation of an unknown unstable system is presented to show the efficacy of the proposed approach.

本文提出了一种新的无模型事件触发的连续时间线性系统最优控制算法。 该问题被表述为无限范围最优自适应学习问题，我们能够同时解决设计控制和触发机制的问题，并通过设计保证最佳性能。 为了提供无模型的解决方案，我们采用 Q-learning 框架和一个批评网络来逼近最优成本，并采用一个零阶持有行动者网络来逼近最优控制。 由于我们有在连续和离散时间内演化的动力学，我们将闭环系统写为脉冲模型并证明其平衡的渐近稳定性。 给出了一个未知不稳定系统的数值模拟，以显示所提出方法的有效性。

1. Introduction
2. Problem formulation
3. Connection between the time-triggered and the event-triggered LQR
4. Model free formulation
5. Simulation
6. Conclusion
Appendix.
References

---

> 以下内容来自CDC | "reinforcement learning" + "event"

只有一篇，在上面已经出现过了。

---

> 以下内容来自CDC | "reinforcement learning" + "event"

无

